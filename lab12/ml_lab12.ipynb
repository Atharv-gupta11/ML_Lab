{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "InaAW55u8AqV",
        "outputId": "ac72fa74-9011-473a-9d9b-ef46532c839d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7255c524-3149-49c3-91ab-509b40529c66\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7255c524-3149-49c3-91ab-509b40529c66\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Score.csv to Score (1).csv\n",
            "Saving Iris.csv to Iris (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter, defaultdict\n",
        "from math import log, exp\n",
        "import random\n",
        "from itertools import combinations\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "metadata": {
        "id": "etzmiX4i_FAB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "def train_val_test_split(X, y, train_frac=0.8, val_frac=0.1, seed=RANDOM_SEED):\n",
        "    assert len(X) == len(y)\n",
        "    n = len(X)\n",
        "    idx = np.arange(n)\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(idx)\n",
        "    Xs = X[idx]\n",
        "    ys = y[idx]\n",
        "    n_train = int(n * train_frac)\n",
        "    n_val = int(n * val_frac)\n",
        "    X_train = Xs[:n_train]\n",
        "    y_train = ys[:n_train]\n",
        "    X_val = Xs[n_train:n_train+n_val]\n",
        "    y_val = ys[n_train:n_train+n_val]\n",
        "    X_test = Xs[n_train+n_val:]\n",
        "    y_test = ys[n_train+n_val:]\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    # ========== SIMPLE PREPROCESS =============\n",
        "def encode_labels(y):\n",
        "    classes, inv = np.unique(y, return_inverse=True)\n",
        "    return inv, {i:cls for i,cls in enumerate(classes)}\n"
      ],
      "metadata": {
        "id": "nJ1yxGpb_Dh7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Q1/Q2: DECISION STUMP (from scratch) ==========\n",
        "class DecisionStump:\n",
        "    \"\"\"\n",
        "    Axis-aligned decision stump for multiclass classification.\n",
        "    Learns a threshold on one feature and assigns a class label to each side.\n",
        "    Supports weighted training (sample weights).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.left_class = None\n",
        "        self.right_class = None\n",
        "        self.polarity = 1  # unused but kept\n",
        "        self.default_class = None\n",
        "\n",
        "    def fit(self, X, y, sample_weights=None, n_thresholds=20):\n",
        "        # X: (n_samples, n_features), y: integer class labels 0..K-1\n",
        "        n, d = X.shape\n",
        "        if sample_weights is None:\n",
        "            sample_weights = np.ones(n) / n\n",
        "        best_err = float('inf')\n",
        "        self.default_class = Counter(y).most_common(1)[0][0]\n",
        "        # For each feature, try thresholds\n",
        "        for j in range(d):\n",
        "            vals = X[:, j]\n",
        "            # consider thresholds = quantiles to speed up\n",
        "            thresholds = np.unique(np.percentile(vals, np.linspace(0, 100, n_thresholds)))\n",
        "            for t in thresholds:\n",
        "                # left = vals <= t, right = vals > t\n",
        "                left_idx = vals <= t\n",
        "                right_idx = ~left_idx\n",
        "                if left_idx.sum() == 0 or right_idx.sum() == 0:\n",
        "                    continue\n",
        "                # find weighted majority class in left and right\n",
        "                def weighted_majority(idx):\n",
        "                    if idx.sum() == 0:\n",
        "                        return self.default_class\n",
        "                    w = defaultdict(float)\n",
        "                    for k,ww in zip(y[idx], sample_weights[idx]):\n",
        "                        w[int(k)] += float(ww)\n",
        "                    return max(w.items(), key=lambda x: x[1])[0]\n",
        "                left_class = weighted_majority(left_idx)\n",
        "                right_class = weighted_majority(right_idx)\n",
        "                # compute weighted error\n",
        "                preds = np.where(left_idx, left_class, right_class)\n",
        "                err = np.sum(sample_weights * (preds != y))\n",
        "                if err < best_err:\n",
        "                    best_err = err\n",
        "                    self.feature_index = j\n",
        "                    self.threshold = t\n",
        "                    self.left_class = left_class\n",
        "                    self.right_class = right_class\n",
        "        # If we didn't find any split, set to majority class\n",
        "        if self.feature_index is None:\n",
        "            self.feature_index = 0\n",
        "            self.threshold = X[:,0].mean()\n",
        "            self.left_class = self.default_class\n",
        "            self.right_class = self.default_class\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        vals = X[:, self.feature_index]\n",
        "        return np.where(vals <= self.threshold, self.left_class, self.right_class)\n",
        "\n",
        "    def predict_proba(self, X, y_train=None, sample_weights=None):\n",
        "        # We'll approximate class probability by returning one-hot of predicted class\n",
        "        preds = self.predict(X)\n",
        "        K = int(max(preds.max(), 0) + 1) if y_train is None else int(np.max(y_train)+1)\n",
        "        proba = np.zeros((len(X), K))\n",
        "        for i,c in enumerate(preds):\n",
        "            proba[i, int(c)] = 1.0\n",
        "        return proba"
      ],
      "metadata": {
        "id": "tRvZufwU-TWn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== AdaBoost - SAMME (discrete multiclass) - from scratch =========\n",
        "class AdaBoostSAMME:\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learners = []\n",
        "        self.alphas = []\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples = X.shape[0]\n",
        "        self.classes_ = np.unique(y)\n",
        "        K = len(self.classes_)\n",
        "        # Initialize weights\n",
        "        w = np.ones(n_samples) / n_samples\n",
        "        for m in range(self.n_estimators):\n",
        "            stump = DecisionStump()\n",
        "            stump.fit(X, y, sample_weights=w, n_thresholds=25)\n",
        "            pred = stump.predict(X)\n",
        "            # weighted error\n",
        "            incorrect = (pred != y).astype(float)\n",
        "            err = np.sum(w * incorrect) / np.sum(w)\n",
        "            # avoid divide by zero / numeric problems\n",
        "            err = max(1e-10, min(1-1e-10, err))\n",
        "            # alpha for SAMME (discrete multiclass)\n",
        "            alpha = np.log((1 - err) / err) + np.log(K - 1)\n",
        "            # update weights\n",
        "            w = w * np.exp(alpha * incorrect)\n",
        "            w = w / np.sum(w)\n",
        "            self.learners.append(stump)\n",
        "            self.alphas.append(alpha)\n",
        "        self.alphas = np.array(self.alphas)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # compute class scores\n",
        "        K = len(self.classes_)\n",
        "        scores = np.zeros((X.shape[0], K))\n",
        "        for alpha, learner in zip(self.alphas, self.learners):\n",
        "            preds = learner.predict(X).astype(int)\n",
        "            for i,p in enumerate(preds):\n",
        "                scores[i, p] += alpha\n",
        "        return np.argmax(scores, axis=1)"
      ],
      "metadata": {
        "id": "ypg9k4yL-X5L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= AdaBoost SAMME.R-like (Real AdaBoost multiclass) ==========\n",
        "class AdaBoostSAMMER:\n",
        "    \"\"\"\n",
        "    Implements a SAMME.R-like real-valued AdaBoost for multiclass.\n",
        "    We use the stump's 'probability' estimate from training set leaves to produce log odds.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learners = []\n",
        "        self.estimator_class_probas = []  # for each learner, mapping from leaf to class prob\n",
        "        self.classes_ = None\n",
        "        self.alphas = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n = X.shape[0]\n",
        "        self.classes_ = np.unique(y)\n",
        "        K = len(self.classes_)\n",
        "        w = np.ones(n) / n\n",
        "        for m in range(self.n_estimators):\n",
        "            stump = DecisionStump()\n",
        "            # For real boosting, when training stump we need class probabilities on each leaf weighted by w.\n",
        "            stump.fit(X, y, sample_weights=w, n_thresholds=25)\n",
        "            # compute per-sample class probability estimates using weighted majority on each leaf\n",
        "            # We'll build mapping: for values <= thresh -> distribution over classes; > thresh -> distribution\n",
        "            feat = stump.feature_index\n",
        "            th = stump.threshold\n",
        "            left_idx = X[:,feat] <= th\n",
        "            right_idx = ~left_idx\n",
        "            # helper to compute weighted class probs\n",
        "            def class_prob(idx):\n",
        "                prob = defaultdict(float)\n",
        "                s = np.sum(w[idx]) if np.sum(w[idx])>0 else 1.0\n",
        "                for lab, wt in zip(y[idx], w[idx]):\n",
        "                    prob[int(lab)] += wt\n",
        "                for k in list(prob.keys()):\n",
        "                    prob[k] /= s\n",
        "                return prob\n",
        "            left_prob = class_prob(left_idx)\n",
        "            right_prob = class_prob(right_idx)\n",
        "            # store these distributions\n",
        "            self.estimator_class_probas.append((left_prob, right_prob, feat, th))\n",
        "            # For prediction on training set produce probs per sample\n",
        "            probs = np.zeros((n, K))\n",
        "            for i in range(n):\n",
        "                if left_idx[i]:\n",
        "                    for k,v in left_prob.items():\n",
        "                        probs[i,k] = v\n",
        "                else:\n",
        "                    for k,v in right_prob.items():\n",
        "                        probs[i,k] = v\n",
        "            # avoid zeros\n",
        "            probs = np.clip(probs, 1e-12, 1.0)\n",
        "            # compute pseudo-response: (K-1)/K * (log p_k - average log p)\n",
        "            # Following Friedman et al. derivation for SAMME.R, alpha_m = 1\n",
        "            # We'll compute contribution to class scores as 0.5 * log(p_k)\n",
        "            # Simpler stable approach: compute estimator's class score for each sample: s_k = log(p_k)\n",
        "            scores = np.log(probs)\n",
        "            # compute the loss/error to update weights\n",
        "            # exponent factor for sample i :\n",
        "            # w_i <- w_i * exp(- (1/K) * sum_k y_{i,k} * s_{i,k}) but we use simpler: increase weight when true class has low p\n",
        "            true_probs = probs[np.arange(n), y]\n",
        "            err = 1 - np.sum(w * true_probs) / np.sum(w)\n",
        "            err = max(1e-12, min(1-1e-12, err))\n",
        "            # set alpha proportional to 0.5*log((1-err)/err) as proxy\n",
        "            alpha = 0.5 * np.log((1-err)/err)\n",
        "            # update weights using class probability on true class (raise weight where prob low)\n",
        "            w = w * np.exp(-alpha * (scores[np.arange(n), y] - np.log(1.0/K)))\n",
        "            w = w / np.sum(w)\n",
        "            self.learners.append(stump)\n",
        "            self.alphas.append(alpha)\n",
        "        self.alphas = np.array(self.alphas)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        K = len(self.classes_)\n",
        "        n = X.shape[0]\n",
        "        scores = np.zeros((n, K))\n",
        "        for alpha, (left_prob, right_prob, feat, th) in zip(self.alphas, self.estimator_class_probas):\n",
        "            # construct per-sample scores = alpha * log p_k\n",
        "            probs = np.zeros((n, K))\n",
        "            left_idx = X[:,feat] <= th\n",
        "            for i in range(n):\n",
        "                if left_idx[i]:\n",
        "                    for k,v in left_prob.items():\n",
        "                        probs[i,k] = v\n",
        "                else:\n",
        "                    for k,v in right_prob.items():\n",
        "                        probs[i,k] = v\n",
        "            probs = np.clip(probs, 1e-12, 1.0)\n",
        "            scores += alpha * np.log(probs)\n",
        "        return np.argmax(scores, axis=1)"
      ],
      "metadata": {
        "id": "j6rbAFqA_bHg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= EVALUATION METRICS: accuracy & classification report ========\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "def classification_report_simple(y_true, y_pred, label_map=None):\n",
        "    labels = np.unique(np.concatenate([y_true, y_pred]))\n",
        "    out = {}\n",
        "    for lab in labels:\n",
        "        tp = np.sum((y_pred==lab)&(y_true==lab))\n",
        "        fp = np.sum((y_pred==lab)&(y_true!=lab))\n",
        "        fn = np.sum((y_pred!=lab)&(y_true==lab))\n",
        "        prec = tp/(tp+fp) if tp+fp>0 else 0.0\n",
        "        rec = tp/(tp+fn) if tp+fn>0 else 0.0\n",
        "        f1 = 2*prec*rec/(prec+rec) if prec+rec>0 else 0.0\n",
        "        out[lab] = {\"precision\":prec, \"recall\":rec, \"f1\":f1, \"support\":np.sum(y_true==lab)}\n",
        "    return out"
      ],
      "metadata": {
        "id": "tlo44UXY_cia"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Q3: DBSCAN from scratch ============\n",
        "def euclidean(a,b):\n",
        "    return np.sqrt(((a-b)**2).sum())\n",
        "\n",
        "def dbscan(X, eps=0.5, min_samples=5):\n",
        "    n = len(X)\n",
        "    labels = -1 * np.ones(n, dtype=int)\n",
        "    visited = np.zeros(n, dtype=bool)\n",
        "    cluster_id = 0\n",
        "    # precompute neighbors via vectorized approach for speed (distance matrix)\n",
        "    # but careful with memory; for moderate datasets it's OK (Iris small). For bigger use KDTree.\n",
        "    D = np.sqrt(((X[:,None,:] - X[None,:,:])**2).sum(axis=2))\n",
        "    for i in range(n):\n",
        "        if visited[i]:\n",
        "            continue\n",
        "        visited[i] = True\n",
        "        neighbors = np.where(D[i] <= eps)[0]\n",
        "        if len(neighbors) < min_samples:\n",
        "            labels[i] = -1  # noise (may be changed later)\n",
        "        else:\n",
        "            # create new cluster\n",
        "            labels[i] = cluster_id\n",
        "            seeds = list(neighbors[neighbors != i])\n",
        "            while seeds:\n",
        "                s = seeds.pop(0)\n",
        "                if not visited[s]:\n",
        "                    visited[s] = True\n",
        "                    neigh_s = np.where(D[s] <= eps)[0]\n",
        "                    if len(neigh_s) >= min_samples:\n",
        "                        # add new neighbors to seeds if not already in seeds\n",
        "                        for nb in neigh_s:\n",
        "                            if nb not in seeds:\n",
        "                                seeds.append(nb)\n",
        "                if labels[s] == -1:\n",
        "                    labels[s] = cluster_id\n",
        "                elif labels[s] is None:\n",
        "                    labels[s] = cluster_id\n",
        "            cluster_id += 1\n",
        "    # some labels may be -1 already; others are >=0\n",
        "    return labels\n",
        "\n",
        "# ======= Silhouette Score from scratch ==========\n",
        "def silhouette_score(X, labels):\n",
        "    # labels: -1 for noise; compute silhouette for non-noise only\n",
        "    n = len(X)\n",
        "    unique_labels = [l for l in np.unique(labels) if l!=-1]\n",
        "    if len(unique_labels) < 2:\n",
        "        return None\n",
        "    D = np.sqrt(((X[:,None,:] - X[None,:,:])**2).sum(axis=2))\n",
        "    sil_samples = []\n",
        "    for i in range(n):\n",
        "        if labels[i] == -1:\n",
        "            continue\n",
        "        same = np.where(labels == labels[i])[0]\n",
        "        other_labels = [l for l in unique_labels if l != labels[i]]\n",
        "        if len(same) == 1:\n",
        "            a = 0.0\n",
        "        else:\n",
        "            a = np.mean(D[i, same[same != i]])\n",
        "        b_vals = []\n",
        "        for ol in other_labels:\n",
        "            ol_idx = np.where(labels == ol)[0]\n",
        "            if len(ol_idx) > 0:\n",
        "                b_vals.append(np.mean(D[i, ol_idx]))\n",
        "        b = min(b_vals) if b_vals else 0.0\n",
        "        denom = max(a,b)\n",
        "        if denom == 0:\n",
        "            sil = 0.0\n",
        "        else:\n",
        "            sil = (b - a) / denom\n",
        "        sil_samples.append(sil)\n",
        "    if len(sil_samples) == 0:\n",
        "        return None\n",
        "    return float(np.mean(sil_samples))\n",
        "\n",
        "# ======= Adjusted Rand Index (from scratch) ==========\n",
        "def adjusted_rand_index(labels_true, labels_pred):\n",
        "    # based on combinatorial formula\n",
        "    n = len(labels_true)\n",
        "    # build contingency table\n",
        "    label_true_ids = {}\n",
        "    label_pred_ids = {}\n",
        "    for i,l in enumerate(labels_true):\n",
        "        label_true_ids.setdefault(l, []).append(i)\n",
        "    for i,l in enumerate(labels_pred):\n",
        "        label_pred_ids.setdefault(l, []).append(i)\n",
        "    # contingency matrix counts\n",
        "    import math\n",
        "    sum_comb_c = 0.0\n",
        "    sum_comb_k = 0.0\n",
        "    for key, idxs in label_true_ids.items():\n",
        "        c = len(idxs)\n",
        "        sum_comb_c += math.comb(c,2) if c>=2 else 0\n",
        "    for key, idxs in label_pred_ids.items():\n",
        "        k = len(idxs)\n",
        "        sum_comb_k += math.comb(k,2) if k>=2 else 0\n",
        "    # sum of combinations for intersections\n",
        "    sum_comb = 0.0\n",
        "    for ct, idxs_t in label_true_ids.items():\n",
        "        set_t = set(idxs_t)\n",
        "        for cp, idxs_p in label_pred_ids.items():\n",
        "            inter = len(set_t.intersection(idxs_p))\n",
        "            if inter>=2:\n",
        "                sum_comb += math.comb(inter,2)\n",
        "    total_comb = math.comb(n,2)\n",
        "    expected_index = (sum_comb_c * sum_comb_k) / total_comb if total_comb>0 else 0\n",
        "    max_index = 0.5 * (sum_comb_c + sum_comb_k)\n",
        "    ari = (sum_comb - expected_index) / (max_index - expected_index) if (max_index - expected_index) != 0 else 0.0\n",
        "    return ari\n",
        "\n",
        "# ========== Q4: Perceptron / Neural nets from scratch ===========\n",
        "def one_hot(y, K=None):\n",
        "    if K is None:\n",
        "        K = len(np.unique(y))\n",
        "    arr = np.zeros((len(y), K))\n",
        "    for i,lab in enumerate(y):\n",
        "        arr[i, lab] = 1\n",
        "    return arr\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    ez = np.exp(z)\n",
        "    return ez / np.sum(ez, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(probs, y_true_onehot):\n",
        "    # probs: (n,K)\n",
        "    n = probs.shape[0]\n",
        "    probs = np.clip(probs, 1e-12, 1-1e-12)\n",
        "    return -np.sum(y_true_onehot * np.log(probs)) / n\n",
        "\n",
        "class MLP:\n",
        "    def __init__(self, layer_sizes, lr=0.01, epochs=200, batch_size=16, seed=RANDOM_SEED):\n",
        "        # layer_sizes: e.g. [D, H, K] where D input dim, K output dim\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.params = {}\n",
        "        np.random.seed(seed)\n",
        "        # initialize weights\n",
        "        for i in range(len(layer_sizes)-1):\n",
        "            in_dim = layer_sizes[i]\n",
        "            out_dim = layer_sizes[i+1]\n",
        "            # Xavier init\n",
        "            limit = np.sqrt(6.0/(in_dim+out_dim))\n",
        "            self.params['W'+str(i+1)] = np.random.uniform(-limit, limit, (in_dim, out_dim))\n",
        "            self.params['b'+str(i+1)] = np.zeros((1, out_dim))\n",
        "\n",
        "    def _forward(self, X):\n",
        "        caches = {}\n",
        "        A = X\n",
        "        L = len(self.layer_sizes)-1\n",
        "        for l in range(1, L+1):\n",
        "            W = self.params['W'+str(l)]\n",
        "            b = self.params['b'+str(l)]\n",
        "            Z = A.dot(W) + b  # (n, out_dim)\n",
        "            if l < L:\n",
        "                A = np.tanh(Z)   # hidden activation\n",
        "            else:\n",
        "                A = softmax(Z)   # last layer softmax\n",
        "            caches['A'+str(l-1)] = (None if l==1 else caches['A'+str(l-2)][0])  # not used\n",
        "            caches['Z'+str(l)] = Z\n",
        "            caches['A'+str(l)] = A\n",
        "        return A, caches\n",
        "\n",
        "    def _backward(self, X, y_onehot):\n",
        "        grads = {}\n",
        "        n = X.shape[0]\n",
        "        L = len(self.layer_sizes)-1\n",
        "        # forward to get activations step by step since we didn't save them nicely\n",
        "        A = [None]*(L+1)\n",
        "        Z = [None]*(L+1)\n",
        "        A[0] = X\n",
        "        for l in range(1, L+1):\n",
        "            W = self.params['W'+str(l)]\n",
        "            b = self.params['b'+str(l)]\n",
        "            Z[l] = A[l-1].dot(W) + b\n",
        "            if l < L:\n",
        "                A[l] = np.tanh(Z[l])\n",
        "            else:\n",
        "                A[l] = softmax(Z[l])\n",
        "        # gradient on output\n",
        "        dA = (A[L] - y_onehot) / n  # (n,K)\n",
        "        for l in reversed(range(1, L+1)):\n",
        "            dZ = dA\n",
        "            grads['W'+str(l)] = A[l-1].T.dot(dZ)\n",
        "            grads['b'+str(l)] = np.sum(dZ, axis=0, keepdims=True)\n",
        "            if l-1 > 0:\n",
        "                dA = dZ.dot(self.params['W'+str(l)].T) * (1 - np.tanh(Z[l-1])**2)\n",
        "            else:\n",
        "                dA = dZ.dot(self.params['W'+str(l)].T)\n",
        "        return grads\n",
        "\n",
        "    def fit(self, X, y, X_val=None, y_val=None, verbose=False):\n",
        "        n = X.shape[0]\n",
        "        K = self.layer_sizes[-1]\n",
        "        y_onehot = one_hot(y, K)\n",
        "        for epoch in range(self.epochs):\n",
        "            # shuffle\n",
        "            idx = np.arange(n)\n",
        "            np.random.shuffle(idx)\n",
        "            for start in range(0, n, self.batch_size):\n",
        "                batch_idx = idx[start:start+self.batch_size]\n",
        "                xb = X[batch_idx]\n",
        "                yb = y_onehot[batch_idx]\n",
        "                # forward\n",
        "                probs, _ = self._forward(xb)\n",
        "                # backward\n",
        "                grads = self._backward(xb, yb)\n",
        "                # update\n",
        "                for k in grads:\n",
        "                    self.params[k] -= self.lr * grads[k]\n",
        "            if verbose and (epoch%50==0 or epoch==self.epochs-1):\n",
        "                train_pred = self.predict(X)\n",
        "                train_acc = np.mean(train_pred == y)\n",
        "                if X_val is not None:\n",
        "                    val_pred = self.predict(X_val)\n",
        "                    val_acc = np.mean(val_pred == y_val)\n",
        "                    print(f\"Epoch {epoch}: train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch}: train_acc={train_acc:.4f}\")\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs, _ = self._forward(X)\n",
        "        return probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return np.argmax(probs, axis=1)"
      ],
      "metadata": {
        "id": "NUTdDf6H_rby"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== RUN: Q1 & Q2 on Credit Score CSV ==========\n",
        "# Load Score.csv from /content (you uploaded file to Colab)\n",
        "score_df = pd.read_csv('/content/Score.csv')  # change path if needed\n",
        "# Simple preprocessing: label encode categorical columns\n",
        "for col in score_df.columns:\n",
        "    if score_df[col].dtype == 'object':\n",
        "        score_df[col] = pd.factorize(score_df[col])[0]\n",
        "# assume the label column is named 'Credit_Score' or last column\n",
        "if 'Credit_Score' in score_df.columns:\n",
        "    label_col = 'Credit_Score'\n",
        "else:\n",
        "    label_col = score_df.columns[-1]\n",
        "X = score_df.drop(label_col, axis=1).values.astype(float)\n",
        "y_raw = score_df[label_col].values\n",
        "y, inv_map = encode_labels(y_raw)\n",
        "\n",
        "# split\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y, train_frac=0.8, val_frac=0.1, seed=RANDOM_SEED)\n",
        "print(\"Score dataset shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "\n",
        "# Train SAMME (discrete)\n",
        "samme = AdaBoostSAMME(n_estimators=100)\n",
        "samme.fit(X_train, y_train)\n",
        "y_pred_test_samme = samme.predict(X_test)\n",
        "print(\"SAMME Test accuracy:\", accuracy(y_test, y_pred_test_samme))\n",
        "print(\"SAMME classification report:\", classification_report_simple(y_test, y_pred_test_samme))\n",
        "\n",
        "# Train SAMME.R-like (real)\n",
        "sammer = AdaBoostSAMMER(n_estimators=100)\n",
        "sammer.fit(X_train, y_train)\n",
        "y_pred_test_sammer = sammer.predict(X_test)\n",
        "print(\"SAMME.R-like Test accuracy:\", accuracy(y_test, y_pred_test_sammer))\n",
        "print(\"SAMME.R-like classification report:\", classification_report_simple(y_test, y_pred_test_sammer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O36Ri7TA_sz6",
        "outputId": "e535e67b-4c3b-4222-b570-e38aaf88bbd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score dataset shapes: (79968, 20) (9996, 20) (9996, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== RUN: Q3 DBSCAN & silhouette on Iris ==========\n",
        "iris_df = pd.read_csv('/content/Iris.csv')  # change path if needed\n",
        "# Some Iris CSV from Kaggle has a column 'Id' or unnamed first; drop non-feature cols\n",
        "possible_label_cols = ['Species','species','class','label']\n",
        "label_col_iris = None\n",
        "for c in iris_df.columns:\n",
        "    if c in possible_label_cols:\n",
        "        label_col_iris = c\n",
        "        break\n",
        "if label_col_iris is None:\n",
        "    # assume last column is label\n",
        "    label_col_iris = iris_df.columns[-1]\n",
        "# features are numeric columns\n",
        "feature_cols = [c for c in iris_df.columns if c != label_col_iris]\n",
        "X_iris = iris_df[feature_cols].values.astype(float)\n",
        "y_iris_raw = iris_df[label_col_iris].values\n",
        "y_iris, iris_map = encode_labels(y_iris_raw)\n",
        "\n",
        "# Standardize features\n",
        "X_iris = (X_iris - X_iris.mean(axis=0)) / (X_iris.std(axis=0) + 1e-12)\n",
        "\n",
        "# Run DBSCAN\n",
        "eps = 0.5\n",
        "min_samples = 5\n",
        "labels_db = dbscan(X_iris, eps=eps, min_samples=min_samples)\n",
        "print(\"DBSCAN labels found:\", np.unique(labels_db))\n",
        "ari = adjusted_rand_index(y_iris, labels_db)\n",
        "sil = silhouette_score(X_iris, labels_db)\n",
        "print(\"DBSCAN ARI:\", ari)\n",
        "print(\"DBSCAN Silhouette:\", sil)\n"
      ],
      "metadata": {
        "id": "hz-TJ82m_yPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== RUN: Q4 Perceptrons (0,1,2 hidden layers) on Iris ==========\n",
        "# split iris into train/test\n",
        "Xtr, Xv, Xt, ytr, yv, yt = train_val_test_split(X_iris, y_iris, train_frac=0.8, val_frac=0.0, seed=RANDOM_SEED)\n",
        "# note: we used val_frac=0.0 to get 80:0:20; modify if you want val set\n",
        "D = Xtr.shape[1]\n",
        "K = len(np.unique(y_iris))\n",
        "\n",
        "# 0 hidden -> logistic softmax (MLP with no hidden layers means W shape (D,K))\n",
        "model0 = MLP([D, K], lr=0.05, epochs=400, batch_size=16)\n",
        "model0.fit(Xtr, ytr, X_val=Xt, y_val=yt, verbose=False)\n",
        "acc0 = accuracy(yt, model0.predict(Xt))\n",
        "\n",
        "# 1 hidden\n",
        "model1 = MLP([D, 8, K], lr=0.05, epochs=400, batch_size=16)\n",
        "model1.fit(Xtr, ytr, X_val=Xt, y_val=yt, verbose=False)\n",
        "acc1 = accuracy(yt, model1.predict(Xt))\n",
        "\n",
        "# 2 hidden\n",
        "model2 = MLP([D, 8, 8, K], lr=0.05, epochs=400, batch_size=16)\n",
        "model2.fit(Xtr, ytr, X_val=Xt, y_val=yt, verbose=False)\n",
        "acc2 = accuracy(yt, model2.predict(Xt))\n",
        "\n",
        "print(\"Perceptron 0 hidden acc:\", acc0)\n",
        "print(\"Perceptron 1 hidden acc:\", acc1)\n",
        "print(\"Perceptron 2 hidden acc:\", acc2)"
      ],
      "metadata": {
        "id": "Mkz_6mVB_3xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(\"CreditScore SAMME test acc:\", accuracy(y_test, y_pred_test_samme))\n",
        "print(\"CreditScore SAMME.R-like test acc:\", accuracy(y_test, y_pred_test_sammer))\n",
        "print(\"Iris DBSCAN ARI:\", ari, \"Silhouette:\", sil)\n",
        "print(\"Iris perceptron accuracies (0,1,2 hidden):\", acc0, acc1, acc2)"
      ],
      "metadata": {
        "id": "s5UbLtRXAFYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}